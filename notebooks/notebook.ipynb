{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751d5e5d-f8a4-4d19-a5c4-38c12ef6fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import string\n",
    "import emoji\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "vectorizer = TfidfVectorizer(stop_words=russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31382fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    return ' '.join([morph.parse(word)[0].normal_form for word in text.split()])\n",
    "punctuation = string.punctuation\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "train_df['text'] = train_df['text'].apply(lambda x: x.lower())\n",
    "train_df['text'] = train_df['text'].apply(lambda x: x.translate(str.maketrans('', '', punctuation)))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "train_df['text'] = train_df['text'].apply(lemmatize_text)\n",
    "test_df['text'] = test_df['text'].apply(lambda x: x.lower())\n",
    "test_df['text'] = test_df['text'].apply(lambda x: x.translate(str.maketrans('', '', punctuation)))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "test_df['text'] = test_df['text'].apply(lemmatize_text)\n",
    "X_train = train_df['text']\n",
    "X_test = test_df['text']\n",
    "Y_train = train_df['label']\n",
    "Y_test = test_df['label']\n",
    "vectorizer = TfidfVectorizer(analyser='char', ngram_range=(4, 6), stop_words=russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a97d778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         видимо в разный регион называть по разный в ро...\n",
      "1         понятно что это нарушение правило писать капсл...\n",
      "2                             какой классный жизненный стих\n",
      "3                              а и правдакогда он запретить\n",
      "4         в солёный вода вирус жить учёный изучать соста...\n",
      "                                ...                        \n",
      "223456    вова дима когда же вы подавиться деньга челове...\n",
      "223457      какой красота просто нет слово выразить чувство\n",
      "223458    вы пост гаи выставитя на перекрёсток возле 21 ...\n",
      "223459                           как то на лебедь непохожий\n",
      "223460                            интересно чей это самолёт\n",
      "Name: text, Length: 223461, dtype: object\n",
      "0         видимо в разный регион называть по разный в ро...\n",
      "1         понятно что это нарушение правило писать капсл...\n",
      "2                             какой классный жизненный стих\n",
      "3                              а и правдакогда он запретить\n",
      "4         в солёный вода вирус жить учёный изучать соста...\n",
      "                                ...                        \n",
      "223456    вова дима когда же вы подавиться деньга челове...\n",
      "223457      какой красота просто нет слово выразить чувство\n",
      "223458    вы пост гаи выставитя на перекрёсток возле 21 ...\n",
      "223459                           как то на лебедь непохожий\n",
      "223460                            интересно чей это самолёт\n",
      "Name: text, Length: 223461, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [02:25:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845704339176924\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "print(X_train)\n",
    "# model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "# param_dist = {\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "#     'max_depth': [3, 5, 7, 10],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "#     'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "#     'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "#     'gamma': [0, 0.1, 0.3],\n",
    "#     'reg_alpha': [0, 0.1, 0.5],\n",
    "#     'reg_lambda': [0, 0.1, 0.5]\n",
    "# }\n",
    "# scorer = make_scorer(f1_score)\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, scoring=scorer, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "# random_search.fit(X_train_vec, Y_train)\n",
    "# print(random_search.best_params_)\n",
    "params = {'subsample': 0.8, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.7}\n",
    "model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, **params)\n",
    "\n",
    "\n",
    "model.fit(X_train_vec, Y_train)\n",
    "pred = model.predict(X_test_vec)\n",
    "print(f1_score(pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b1afa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m huy \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mруки бы оторвать\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m huy \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhuy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mpredict(huy))\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents):\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2112\u001b[0m \n\u001b[0;32m   2113\u001b[0m \u001b[38;5;124;03m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2126\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe TF-IDF vectorizer is not fitted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2128\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m   2129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1757\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "huy = pd.array(['руки бы оторвать'])\n",
    "huy = vectorizer.transform(huy)\n",
    "print(model.predict(huy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
